{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연산 처리를 위한 패키지\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from math import sqrt\n",
    "\n",
    "# 데이터 분석을 위한 패키지\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 시각화를 위한 패키지\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# 필요모듈 import\n",
    "import os\n",
    "import openpyxl\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 그래프를 실제로 그리기 위한 설정\n",
    "%matplotlib inline\n",
    "\n",
    "# 머신러닝 패키지\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression  # 1. Linear Regression\n",
    "from sklearn.linear_model import Lasso  # 2. Lasso\n",
    "from sklearn.linear_model import Ridge  # 3. Ridge\n",
    "from xgboost.sklearn import XGBRegressor  # 4. XGBoost\n",
    "from lightgbm.sklearn import LGBMRegressor  # 5. LightGBM\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import random\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "# KFold(CV), partial : optuna를 사용하기 위함\n",
    "from sklearn.model_selection import KFold\n",
    "from functools import partial\n",
    "\n",
    "# 폰트 처리\n",
    "# plt.rc('font', family='NanumGothic')        # for windows\n",
    "plt.rc('font', family='AppleGothic') # For MacOS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function sets\n",
    "# feature constructing\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                 3-2-1. 고유번호 끊어주기                  #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "def setting_data(train, test, one, two, three, four, five, six):\n",
    "  test['송하인_격자공간고유번호'] = test['송하인_격자공간고유번호'].astype(str)\n",
    "  test['수하인_격자공간고유번호'] = test['수하인_격자공간고유번호'].astype(str)\n",
    "  train['송하인_격자공간고유번호'] = train['송하인_격자공간고유번호'].astype(str)\n",
    "  train['수하인_격자공간고유번호'] = train['수하인_격자공간고유번호'].astype(str)\n",
    "\n",
    "  train['송하인_코드1'] = train['송하인_격자공간고유번호'].str.slice(int(one),int(two))\n",
    "  train['송하인_코드2'] = train['송하인_격자공간고유번호'].str.slice(int(two),int(three))\n",
    "  train['송하인_코드3'] = train['송하인_격자공간고유번호'].str.slice(int(three),int(four))\n",
    "  train['송하인_코드4'] = train['송하인_격자공간고유번호'].str.slice(int(four),int(five))\n",
    "  train['송하인_코드5'] = train['송하인_격자공간고유번호'].str.slice(int(five),int(six))\n",
    "\n",
    "  train['수하인_코드1'] = train['수하인_격자공간고유번호'].str.slice(int(one),int(two))\n",
    "  train['수하인_코드2'] = train['수하인_격자공간고유번호'].str.slice(int(two),int(three))\n",
    "  train['수하인_코드3'] = train['수하인_격자공간고유번호'].str.slice(int(three),int(four))\n",
    "  train['수하인_코드4'] = train['수하인_격자공간고유번호'].str.slice(int(four),int(five))\n",
    "  train['수하인_코드5'] = train['수하인_격자공간고유번호'].str.slice(int(five),int(six))\n",
    "\n",
    "  test['송하인_코드1'] = test['송하인_격자공간고유번호'].str.slice(int(one),int(two))\n",
    "  test['송하인_코드2'] = test['송하인_격자공간고유번호'].str.slice(int(two),int(three))\n",
    "  test['송하인_코드3'] = test['송하인_격자공간고유번호'].str.slice(int(three),int(four))\n",
    "  test['송하인_코드4'] = test['송하인_격자공간고유번호'].str.slice(int(four),int(five))\n",
    "  test['송하인_코드5'] = test['송하인_격자공간고유번호'].str.slice(int(five),int(six))\n",
    "\n",
    "  test['수하인_코드1'] = test['수하인_격자공간고유번호'].str.slice(int(one),int(two))\n",
    "  test['수하인_코드2'] = test['수하인_격자공간고유번호'].str.slice(int(two),int(three))\n",
    "  test['수하인_코드3'] = test['수하인_격자공간고유번호'].str.slice(int(three),int(four))\n",
    "  test['수하인_코드4'] = test['수하인_격자공간고유번호'].str.slice(int(four),int(five))\n",
    "  test['수하인_코드5'] = test['수하인_격자공간고유번호'].str.slice(int(five),int(six))\n",
    "\n",
    "  train = train[['물품_카테고리', '송하인_코드1','송하인_코드2','송하인_코드3', '송하인_코드4', '송하인_코드5','수하인_코드1', '수하인_코드2', '수하인_코드3', '수하인_코드4', '수하인_코드5','운송장_건수']]\n",
    "\n",
    "  test = test[['물품_카테고리','송하인_코드1','송하인_코드2', '송하인_코드3', '송하인_코드4', '송하인_코드5','수하인_코드1', '수하인_코드2', '수하인_코드3', '수하인_코드4', '수하인_코드5',]]\n",
    "\n",
    "  for col in test.columns:\n",
    "    train[col]=train[col].astype('category')\n",
    "    test[col]=test[col].astype('category')\n",
    "                \n",
    "  return train, test\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                 3-2-2. 카테고리 대분류 추가               #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "def large_category (train, test):\n",
    "    \n",
    "    change_cat_dict = {\n",
    "    ('아우터', '신발', '상의', '하의', '기타패션의류', '양말/스타킹', '잠옷', '원피스/점프슈트') : '의류',\n",
    "    ('농산물') : '농산물',\n",
    "    ('수납가구', '아웃도어가구', '카페트/러그', '서재/사무용가구', '주방가구', '침실가구') : '가구', \n",
    "    ('의료용품', '건강용품', '생활용품', '문구/사무용품', '주방용품', '침구세트', '침구단품', '인테리어소품', '공구', '커튼/블라인드', 'DIY자재/용품', '위생/건강용품', '뷰티소품', '세탁용품', '수납/정리용품', '반려동물', '완구/매트', '홈데코') :  '생활용품',\n",
    "    ('수산', '음료', '기타식품', '건강식품', '가공식품', '축산', '과자', '다이어트식품', '냉동/간편조리식품', '김치', '반찬') : '수산/기타식품',\n",
    "    ('음반', '문화컨텐츠') : '문화',\n",
    "    ('스킨케어', '헤어케어', '바디케어', '클렌징', '기타화장품/미용', '헤어스타일링', '베이스메이크업', '향수', '선케어', '네일케어', '색조메이크업', '남성화장품', '구강위생용품', '눈관리용품', '욕실용품') : '미용',\n",
    "    ('기저귀/물티슈', '분유/이유식/아기간식', '기타출산/육아', '유아가구', '출산/유아동잡화', '출산/유아동의류') : '영유아',\n",
    "    ('기타디지털/가전', '모니터', 'PC', 'PC주변기기', '계절가전', '태블릿PC/노트북액세서리', '스마트디바이스', '스마트디바이스액세서리', '게임기/타이틀', '생활가전', '음향가전', '주방가전', '이미용가전') : '디지털가전',\n",
    "    ('기타스포츠/레저', '등산', '낚시', '스포츠잡화', '캠핑', '헬스', '취미용품', '골프') : '스포츠',\n",
    "    ('헤어액세서리', '모자', '기타패션잡화', '언더웨어', '패션소품', '선글라스/안경테', '지갑', '기능성', '가방', '헤어액세서리', '주얼리', '자동차용품', '재활운동용품') : '패션잡화',\n",
    "    }   \n",
    "\n",
    "    train['물품_카테고리_소분류'] = train['물품_카테고리']\n",
    "    test['물품_카테고리_소분류'] = test['물품_카테고리']\n",
    "    train['물품_카테고리_대분류'] = train['물품_카테고리']\n",
    "    test['물품_카테고리_대분류'] = test['물품_카테고리']\n",
    "\n",
    "    train = train.replace({'물품_카테고리_대분류': change_cat_dict})\n",
    "    test = test.replace({'물품_카테고리_대분류': change_cat_dict})\n",
    "\n",
    "    train = train[['물품_카테고리_대분류','물품_카테고리_소분류', '송하인_코드1','송하인_코드2','송하인_코드3', '송하인_코드4', '송하인_코드5','수하인_코드1', '수하인_코드2', '수하인_코드3', '수하인_코드4', '수하인_코드5','운송장_건수']]\n",
    "\n",
    "    test = test[['물품_카테고리_대분류','물품_카테고리_소분류', '송하인_코드1','송하인_코드2', '송하인_코드3', '송하인_코드4', '송하인_코드5','수하인_코드1', '수하인_코드2', '수하인_코드3', '수하인_코드4', '수하인_코드5',]]\n",
    "    \n",
    "    for col in test.columns:\n",
    "        train[col]=train[col].astype('category')\n",
    "        test[col]=test[col].astype('category')\n",
    "    \n",
    "    return train, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoders\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                     3-3. 인코딩                        #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "\n",
    "# 원핫인코딩, 라벨인코딩 선택\n",
    "\n",
    "## 원핫인코딩\n",
    "def one_hot_encoder(train, test):\n",
    "  categorical_features = train.columns[0:12]\n",
    "  temp = train.columns[13:]\n",
    "  card1 = train.columns[train.nunique() == 1]\n",
    "  binary_features = np.setdiff1d(temp, card1)\n",
    "  print(\"%d features - %d features = %d binary features, %d categorical features\" % (len(temp), len(card1), len(binary_features), len(categorical_features)))\n",
    "\n",
    "  # feature engineering을 위해 tempX, y 생성\n",
    "  total = pd.concat([train, test])\n",
    "  split_point = len(train)\n",
    "  total_OHE = pd.get_dummies(data=total, columns=categorical_features)\n",
    "  y = train.운송장_건수\n",
    "  tempX = total_OHE.drop(columns=[\"운송장_건수\"])\n",
    "  tempX = tempX.drop(columns=card1)\n",
    "  trainX = tempX[:split_point]\n",
    "  testX = tempX[split_point:]\n",
    "  print(trainX.shape, testX.shape, y.shape)\n",
    "\n",
    "  return y, trainX, testX\n",
    "\n",
    "## 라벨인코딩\n",
    "\n",
    "def label_encoder(train, test):\n",
    "\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "  categorical_features = train.columns[0:12]\n",
    "  temp = train.columns[13:]\n",
    "  card1 = train.columns[train.nunique() == 1]\n",
    "  binary_features = np.setdiff1d(temp, card1)\n",
    "  print(\"%d features - %d features = %d binary features, %d categorical features\" % (len(temp), len(card1), len(binary_features), len(categorical_features)))\n",
    "\n",
    "  total = pd.concat([train, test])\n",
    "  split_point = len(train)\n",
    "\n",
    "  LE = LabelEncoder()\n",
    "\n",
    "  total['물품_카테고리_소분류'] = LE.fit_transform(total['물품_카테고리_소분류'])\n",
    "  total['물품_카테고리_대분류'] = LE.fit_transform(total['물품_카테고리_대분류'])\n",
    "  total['송하인_코드1'] = LE.fit_transform(total['송하인_코드1'])\n",
    "  total['송하인_코드2'] = LE.fit_transform(total['송하인_코드2'])\n",
    "  total['송하인_코드3'] = LE.fit_transform(total['송하인_코드3'])\n",
    "  total['송하인_코드4'] = LE.fit_transform(total['송하인_코드4'])\n",
    "  total['송하인_코드5'] = LE.fit_transform(total['송하인_코드5'])\n",
    "  total['수하인_코드1'] = LE.fit_transform(total['수하인_코드1'])\n",
    "  total['수하인_코드2'] = LE.fit_transform(total['수하인_코드2'])\n",
    "  total['수하인_코드3'] = LE.fit_transform(total['수하인_코드3'])\n",
    "  total['수하인_코드4'] = LE.fit_transform(total['수하인_코드4'])\n",
    "  total['수하인_코드5'] = LE.fit_transform(total['수하인_코드5'])\n",
    "\n",
    "  y = train.운송장_건수\n",
    "  tempX = total.drop(columns=[\"운송장_건수\"])\n",
    "  tempX = tempX.drop(columns=card1)\n",
    "  trainX = tempX[:split_point]\n",
    "  testX = tempX[split_point:]\n",
    "  print(trainX.shape, testX.shape, y.shape)\n",
    "\n",
    "  return y, trainX, testX\n",
    "\n",
    "def none (train, test):\n",
    "\n",
    "    categorical_features = train.columns[0:12]\n",
    "    temp = train.columns[13:]\n",
    "    card1 = train.columns[train.nunique() == 1]\n",
    "    binary_features = np.setdiff1d(temp, card1)\n",
    "    print(\"%d features - %d features = %d binary features, %d categorical features\" % (len(temp), len(card1), len(binary_features, len(categorical_features))))\n",
    "    \n",
    "    total = pd.concat([train, test])\n",
    "    split_point = len(train)\n",
    "    y = train.운송장_건수\n",
    "    tempX = total.drop(columns=[\"운송장_건수\"])\n",
    "    tempX = tempX.drop(columns=card1)\n",
    "    trainX = tempX[:split_point]\n",
    "    testX = tempX[split_point:]\n",
    "    print(trainX.shape, testX.shape, y.shape)\n",
    "\n",
    "    return y, trainX, testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Reducer\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                     3-4. 차원축소                      #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "# 3-4. 차원축소\n",
    "\n",
    "def feature_reducer(trainX, feature_reducing):\n",
    "\n",
    "    if feature_reducing == \"correlation\":\n",
    "        threshold = 0.7\n",
    "        correlated_features = remove_collinearity(trainX, threshold)\n",
    "        correlated_features = set(correlated_features) # 중복 제거\n",
    "        print(\"%d Correlation features over %.2f\" % (len(correlated_features), threshold))\n",
    "        \n",
    "        X = trainX.drop(columns=correlated_features)\n",
    "        print(X.shape)\n",
    "        return X, correlated_features\n",
    "        \n",
    "    elif feature_reducing == \"feature_importance\":\n",
    "        show_plot = True\n",
    "        model = RandomForestRegressor(max_features=\"sqrt\", n_jobs=-1, random_state=0xC0FFEE)\n",
    "        model.fit(trainX, y)\n",
    "        important_features = find_feature_importance(trainX, model, show_plot)\n",
    "        X = trainX[important_features]\n",
    "        print(X.shape)\n",
    "        return X, important_features\n",
    "        \n",
    "    elif feature_reducing == \"PCA\":\n",
    "        show_plot = True\n",
    "        pca_model, X = apply_PCA(trainX, show_plot)\n",
    "        print(X.shape)\n",
    "        return X, pca_model\n",
    "\n",
    "\n",
    "## 1. correlation\n",
    "\n",
    "# 중복정보가 있는 column 제거하기 위해 상관계수를 확인해봅니다.\n",
    "def remove_collinearity(X, threshold):\n",
    "    \"\"\"\n",
    "    X : feature matrix\n",
    "    threshold : 다중공선성을 제거할 column을 고르는 기준 값. [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    corr = X.corr()\n",
    "    candidate_cols = []\n",
    "    \n",
    "    for x in corr.iterrows():\n",
    "        idx, row = x[0], x[1] # decoupling tuple\n",
    "        # 해당 row는 이미 처리가 되어서 볼 필요가 없다.\n",
    "        if idx in candidate_cols:\n",
    "            continue\n",
    "        #print(row[row > 0.7].index[1:])\n",
    "        candidates = row[row > threshold].index[1:]\n",
    "\n",
    "        # 자기 자신을 제외하고 threshold를 넘는 column이 있다면,\n",
    "        if len(candidates) != 0:\n",
    "            for col in candidates:\n",
    "                candidate_cols.append(col)           \n",
    "    \n",
    "    return candidate_cols\n",
    "\n",
    "def find_feature_importance(X, model, show_plot):\n",
    "\n",
    "    feat_names = X.columns.values\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    indices = np.argsort(importances)[::-1][:20]\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\n",
    "    plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n",
    "    plt.xlim([-1, len(indices)])\n",
    "    plt.show()\n",
    "    \n",
    "    important_features = X.columns[importances >= 0.005]\n",
    "    return important_features\n",
    "    \n",
    "def apply_PCA(X, show_plot):\n",
    "    from sklearn.decomposition import PCA\n",
    "    # training data와 test data를 모두 PCA를 이용하여 차원 감소를 수행합니다.\n",
    "    pca = PCA(n_components=0.90) # 원래 데이터의 90%를 보존하는 차원.\n",
    "    pca_090 = pca.fit(X) # 학습 및 변환\n",
    "    reduced_X = pca_090.transform(X)\n",
    "    print(reduced_X.shape)\n",
    "    \n",
    "    if show_plot:\n",
    "        labels = [f\"PC{x}\" for x in range(1, reduced_X.shape[1]+1)]\n",
    "        pca_090_variance = np.round(pca_090.explained_variance_ratio_.cumsum()*100, decimals=1)\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.bar(x=range(1, len(pca_090_variance)+1), height=pca_090_variance, tick_label=labels)\n",
    "\n",
    "        plt.xticks(rotation=90, color='indigo', size=15)\n",
    "        plt.yticks(rotation=0, color='indigo', size=15)\n",
    "        plt.title('Scree Plot',color='tab:orange', fontsize=25)\n",
    "        plt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\n",
    "        plt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n",
    "        plt.show()\n",
    "        \n",
    "        X_train_pca_df = pd.DataFrame(reduced_X, columns=labels)\n",
    "        display(X_train_pca_df)\n",
    "\n",
    "    return pca_090, X_train_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling sets\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                     4. Modeling                      #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "\n",
    "## 5가지 모델 세트\n",
    "\n",
    "def model5(X_train, y_train, X_val, y_val):\n",
    "  reg = LinearRegression()\n",
    "  reg2 = Lasso()\n",
    "  reg3 = Ridge()\n",
    "  reg4 = XGBRegressor()\n",
    "  reg5 = LGBMRegressor()\n",
    "\n",
    "  reg.fit(X_train, y_train)\n",
    "  reg2.fit(X_train, y_train)\n",
    "  reg3.fit(X_train, y_train)\n",
    "  reg4.fit(X_train, y_train)\n",
    "  reg5.fit(X_train, y_train)\n",
    "\n",
    "  pred_train = reg.predict(X_train)\n",
    "  pred_train2 = reg2.predict(X_train)\n",
    "  pred_train3 = reg3.predict(X_train)\n",
    "  pred_train4 = reg4.predict(X_train)\n",
    "  pred_train5 = reg5.predict(X_train)\n",
    "\n",
    "  pred_val = reg.predict(X_val)\n",
    "  pred_val2 = reg2.predict(X_val)\n",
    "  pred_val3 = reg3.predict(X_val)\n",
    "  pred_val4 = reg4.predict(X_val)\n",
    "  pred_val5 = reg5.predict(X_val)\n",
    "\n",
    "  mse_train = mean_squared_error(y_train, pred_train)\n",
    "  mse_train2 = mean_squared_error(y_train, pred_train2)\n",
    "  mse_train3 = mean_squared_error(y_train, pred_train3)\n",
    "  mse_train4 = mean_squared_error(y_train, pred_train4)\n",
    "  mse_train5 = mean_squared_error(y_train, pred_train5)\n",
    "\n",
    "  mse_val = mean_squared_error(y_val, pred_val)\n",
    "  mse_val2 = mean_squared_error(y_val, pred_val2)\n",
    "  mse_val3 = mean_squared_error(y_val, pred_val3)\n",
    "  mse_val4 = mean_squared_error(y_val, pred_val4)\n",
    "  mse_val5 = mean_squared_error(y_val, pred_val5)\n",
    "\n",
    "  r2_train = r2_score(y_train, pred_train)\n",
    "  r2_train2 = r2_score(y_train, pred_train2)\n",
    "  r2_train3 = r2_score(y_train, pred_train3)\n",
    "  r2_train4 = r2_score(y_train, pred_train4)\n",
    "  r2_train5 = r2_score(y_train, pred_train5)\n",
    "\n",
    "  r2_val = r2_score(y_val, pred_val)\n",
    "  r2_val2 = r2_score(y_val, pred_val2)\n",
    "  r2_val3 = r2_score(y_val, pred_val3)\n",
    "  r2_val4 = r2_score(y_val, pred_val4)\n",
    "  r2_val5 = r2_score(y_val, pred_val5)\n",
    "\n",
    "  print(\"------- train/val mse -------\")\n",
    "  print(\"1. Linear Regression train/val \\t= %.4f, %.4f\" % (mse_train, mse_val))\n",
    "  print(\"2. Lasso, train/val \\t\\t = %.4f, %.4f\" % (mse_train2, mse_val2))\n",
    "  print(\"3. Ridge, train/val \\t\\t = %.4f, %.4f\" % (mse_train3, mse_val3))\n",
    "  print(\"4. XGBoost, train/val \\t\\t = %.4f, %.4f\" % (mse_train4, mse_val4))\n",
    "  print(\"5. LightGBM, train/val \\t\\t = %.4f, %.4f\" % (mse_train5, mse_val5))\n",
    "\n",
    "  print(\"------- train/val R2 score -------\")\n",
    "  print(\"1. Linear Regression train/val \\t= %.4f, %.4f\" % (r2_train, r2_val))\n",
    "  print(\"2. Lasso, train/val \\t\\t = %.4f, %.4f\" % (r2_train2, r2_val2))\n",
    "  print(\"3. Ridge, train/val \\t\\t = %.4f, %.4f\" % (r2_train3, r2_val3))\n",
    "  print(\"4. XGBoost, train/val \\t\\t = %.4f, %.4f\" % (r2_train4, r2_val4))\n",
    "  print(\"5. LightGBM, train/val \\t\\t = %.4f, %.4f\" % (r2_train5, r2_val5))\n",
    "\n",
    "  return reg, reg2, reg3, reg4, reg5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "#                     4. Modeling                      #\n",
    "\n",
    "\n",
    "\n",
    "# ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ#\n",
    "\n",
    "\n",
    "## grid search\n",
    "\n",
    "## grid serch CV 적용\n",
    "\n",
    "def grid_search(X_train, y_train, regs):\n",
    "  # Grid searchCV\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "  param_grid = {                            \n",
    "    \"max_depth\": [1, 3, 5],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.1, 0.3, 0.5]\n",
    "  }\n",
    "\n",
    "  gcv = GridSearchCV(estimator=regs, param_grid=param_grid, scoring= 'neg_mean_squared_error', verbose = 1)\n",
    "  gcv.fit(X_train, y_train)\n",
    "  print(gcv.best_estimator_)\n",
    "  print(gcv.best_params_)\n",
    "\n",
    "  return gcv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## optuna\n",
    "######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "옵튜나 코드 만들기\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "\n",
    "## prediction - 5models\n",
    "\n",
    "def predict(gcv, X_train, y_train, X_val, X_test, y_test):\n",
    "  # training set과 같은 전처리 : V\n",
    "  final_model = gcv.best_estimator_\n",
    "  final_model.fit(X_train, y_train)\n",
    "  pred_train = final_model.predict(X_train)\n",
    "  pred_val = final_model.predict(X_val)\n",
    "  pred_test = final_model.predict(X_test)\n",
    "\n",
    "    # PCA 적용\n",
    "  if reducer == \"correlation\":\n",
    "      test = testX.drop(columns=reduced)\n",
    "      print(X.shape)\n",
    "      \n",
    "  elif reducer == \"feature_importance\":\n",
    "      test = testX[reduced]\n",
    "      print(X.shape)\n",
    "      \n",
    "  elif reducer == \"PCA\":\n",
    "      test = reduced.transform(testX)\n",
    "      print(X.shape)\n",
    "      \n",
    "  prediction = final_model.predict(test)\n",
    "\n",
    "  mse_train = mean_squared_error(y_train, pred_train)\n",
    "  mse_val = mean_squared_error(y_val, pred_val)\n",
    "  mse_test = mean_squared_error(y_test, pred_test)\n",
    "\n",
    "  r2_train = r2_score(y_train, pred_train)\n",
    "  r2_val = r2_score(y_val, pred_val)\n",
    "  r2_test = r2_score(y_test, pred_test)\n",
    "\n",
    "  print(\"------- train/val mse -------\")\n",
    "\n",
    "  print(\"5. LightGBM, train/val/test \\t\\t = %.4f, %.4f, %.4f\" % (mse_train, mse_val, mse_test) )\n",
    "\n",
    "  print(\"------- train/val R2 score -------\")\n",
    "\n",
    "  print(\"5. LightGBM, train/val/test \\t\\t = %.4f, %.4f, %.4f\" % (r2_train, r2_val, r2_test) )\n",
    "\n",
    "  prediction = final_model.predict(test)\n",
    "  \n",
    "  submission[\"운송장_건수\"] = prediction\n",
    "  \n",
    "  display(submission)\n",
    "\n",
    "  return final_model, pred_train, pred_val, pred_test, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴 데이터 로딩\n",
    "test = pd.read_csv('data/final_test.csv', index_col = 0)             # unnamed_0 이라는 index가 추가되어 나오지 않게 'index_col = 0' 을 추가했습니다.\n",
    "train = pd.read_csv('data/final_train.csv', index_col = 0)\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 데이터에서 값이 잘못되거나 타입이 맞지 않는 행이나 열을 제거하는 작업을 수행합니다.\n",
    "\n",
    "이 데이터에서는 결측치가 없으며, 데이터타입 또한 범주형으로 통일되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요시 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Feature construction\n",
    "\n",
    "> 기존의 피쳐를 기반으로 새로운 피쳐를 만들어냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-1. 고유번호 끊어주기\n",
    "> 이 데이터에서는 격자공간 고유번호의 각 자리수가 단위별로 의미를 가지고 있는 것을 확인했습니다.\n",
    "> \n",
    "> 16자리의 격자공간고유번호를 1~2 / 3~5 / 6~9 / 10 / 11~16 자리로 끊었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = setting_data(train, test, '0', '2', '5', '9', '10', '16')       # 0, 2, 5, 9, 10, 16번째 위치에서 코드를 잘라서 저장합니다. 이걸로 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-2. 카테고리 대분류 추가\n",
    "\n",
    "> 물품 카테고리를 좀 더 큰 항목으로 묶어, 100개의 소분류 카테고리를 10개의 대분류 카테고리로 분류했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = large_category(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원하는 학습 모델 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. 인코딩 Representation transformation      << select_model 선택해줘야함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 원핫, 라벨, 민타겟 인코딩을 통해 데이터를 전처리하는 과정을 진행합니다.\n",
    "> \n",
    "> lightGBM과 Catboost 모델은 인코딩을 하지 않고 돌리는게 성능이 더 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 모델 선택!\n",
    "select_model = '5models' # '5models', 'catboost(미구현)', 'lightgbm(미구현)', 'randomforest(미구현)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택한 모델에 따라 인코딩 선택!\n",
    "\n",
    "if select_model == '5models':\n",
    "  encoder_select = 'label_encoder' # 'label_encoder', 'one_hot_encoder'\n",
    "elif select_model == 'catboost':\n",
    "  encoder_select = 'none'\n",
    "elif select_model == 'lightgbm':\n",
    "  encoder_select = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택된 인코더 실행\n",
    "if encoder_select == \"label_encoder\":\n",
    "  y, trainX, testX = label_encoder(train, test)\n",
    "  \n",
    "elif encoder_select == \"one_hot_encoder\":\n",
    "  y, trainX, testX = one_hot_encoder(train, test)\n",
    "\n",
    "if encoder_select == \"none\":\n",
    "  y, trainX, testX = none(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. 차원 축소 Feature extraction        << reducer 선택해줘야함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 전체 feature의 수를 줄이거나 피쳐를 해시값으로 변환해 효율적인 피쳐를 사용하는 방법을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원축소 방법 선택\n",
    "reducer = \"feature_importance\" # \"correlation\" / \"feature_importance\" / \"PCA\"\n",
    "X, reduced = feature_reducer(trainX, reducer) # \"correlation\" / \"feature_importance\" / \"PCA\" ,  X 는 차원축소된 데이터입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. Data partitioning\n",
    "> 학습, 평가, 테스트용 데이터로 나누는 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "\n",
    "- Model training\n",
    "\n",
    "- Hyper-parameter tuning\n",
    "\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5models 모델 실행 \n",
    "# 1 : linear regression, \n",
    "# 2 : ridge, \n",
    "# 3 : lasso, \n",
    "# 4 : XGBoost, \n",
    "# 5 : lightgbm\n",
    "\n",
    "reg, reg2, reg3, reg4, reg5 = model5(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. hyperparameter tuning  << select_reg 설정해줘야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "select_reg = reg5                                     # reg = linear regression, reg2 = ridge, reg3 = lasso, reg4 = XGBoost, reg5 = lightgbm\n",
    "\n",
    "gcv = grid_search(X_train, y_train, select_reg)       # regressor 모델 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, pred_train, pred_val, pred_test, prediction = predict(gcv, X_train, y_train, X_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 제출파일 생성        << 극단값 조정해줘야함!\n",
    "> submission 극단값 확인, 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[submission.운송장_건수>20,'운송장_건수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[submission.운송장_건수>20,'운송장_건수']=submission.loc[submission.운송장_건수>20,'운송장_건수']*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일명 : \"제출시간 인코더_차원축소방법_(직접입력! 모델명).csv\"\n",
    "\n",
    "import datetime\n",
    "path = \"submissions/\"\n",
    "submission.reset_index(drop=True).to_csv(path + str(datetime.datetime.now()) + f\"{encoder_select}_{reducer}_lgbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19381dae11859029849bf77a2bf763c10e0e1b48ddfabe634499c3aa535e0678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
